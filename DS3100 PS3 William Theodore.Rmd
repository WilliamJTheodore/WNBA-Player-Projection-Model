---
title: "DS3100 PS3"
author: "William Theodore"
date: "2024-03-04"
output:
  pdf_document: default
  html_document: default
---

```{r}
library(tidyverse) 
df <- read.csv('NBA_stats.csv') # Loads in csv file
```

This dataset was obtained from the Kaggle database (https://www.kaggle.com/datasets/sumitredekar/nba-stats-2018-2021). The dataset was scraped from basketball-reference.com It provides valuable insights into NBA player's average for the years 2018 - 2021. The dataset includes 25 columns which consist of the player's name, position, team, games played, year and a variety of average statitics (points, rebounds, assists, etc.). There are 2728 combinations of players and years (some with multiple years) that are included in this dataset. 

## Motivation

# Research Question

How do we best predict points scored per game in the NBA?

# Variables of Interest:

The dependent variable of interest is Points (per game). I think that the dependent variable will best be explained by the overall volume of shots taken by a player per game (two pointers, three pointers, and free throws), the minutes a player plays a game, and the age of the player.

All of the variables of interest are measured numerically. Age is the only variable that is only measured in integers. The rest of the variables are measured to the first decimal place.

- Points (numeric) - The average points per game scored by a player in a year.
- Minutes Played (numeric) - The average minutes played per game by a player in a year.
- Field Goals Attempted (numeric) - The average shots attempted per game by a player in a year.
- Free Throws Attempted (numeric) - The average free throws attempted per game by a player in a year.
- Three Pointers Attempted (numeric) - The average three pointers attempted per game by a player in a year.
- Age (numeric) - The age of the player in a given year.

# Hypothesis


$$H_0: \beta_1 = \beta_2, = \beta_3 = \beta_4 = \beta_5 = 0 \\$$
$$H_1: Any \ \beta_i \neq 0$$


The null hypothesis is that none of the explanatory variables have a significant effect on points scored per game. The alternative hypothesis is that any of the coefficients of the independent variables are not equal to 0 (they are significant).

## Data Preparation

# Visualization

```{r}
df |>
  ggplot(aes(x = Minutes.Played, y = Points )) + # Plots points by minutes played
  geom_point() + # Creates a scatter plot
  geom_smooth(method = "lm") +# Adds a linear line of best fit
  labs(title = "Points by Minutes Played", x = "Minutes Played", y = "Points")
```

```{r}
df |>
  ggplot(aes(x = Fields.Goal.Attempted, y = Points )) + # Plots points by field goals attempted
  geom_point() + # Creates a scatter plot
  geom_smooth(method = "lm") + # Adds a linear line of best fit
  labs(title = "Points by Field Goals Attempts", x = "Field Goals Attempted ", y = "Points")
```

```{r}
df |>
  ggplot(aes(x = Free.Throws.Attempted, y = Points )) + # Plots points by free throws attempted
  geom_point() +# Creates a scatter plot
  geom_smooth(method = "lm") + # Adds a linear line of best fit
  labs(title = "Points by Free Throw Attempts", x = "Free Throws Attempted", y = "Points")
```

```{r}
df |>
  ggplot(aes(x = X3.points.Field.Goal.Attempted, y = Points )) + # Plots points by three point field goals attempted
  geom_point() + # Creates a scatter plot
  geom_smooth(method = "lm") + # Adds a linear line of best fit
  labs(title = "Points by Three Point Attempts", x = "Three Points Attempted", y = "Points" )
```

```{r}
df |>
  ggplot(aes(x = Age, y = Points )) + # Plots points by age
  geom_histogram(stat = "identity") + # Creates a histogram
  labs(title = "Points by Age", x = "Age", y = "Points")
```

To see the relationships between the variables of interest and the points scored per game of NBA players, I visualized each with a graph. From the visualizations, it is clear that the Minutes Played, Field Goals Attempted, and Free Throws Attempted all have very strong correlation with points scored per game, Three Pointer Attempts per game still has moderate correlation, but not as strong as the others. Furthermore, the histogram of Points per game by Age is slightly skewed to the right. All of the independent variables of interest are about linear to the dependent variable, which suggests a linear regression model is most suitable.

# Data Wrangling

```{r}
df <- df |>
  rename(Threes.Attempted = X3.points.Field.Goal.Attempted) |> # Renames threes column for practical purposes (readibility)
  dplyr::mutate(Twos.Attempted = Fields.Goal.Attempted - Threes.Attempted) |> 
  dplyr::select(Points, Minutes.Played, Free.Throws.Attempted, Threes.Attempted, Twos.Attempted, Age) # Filters model to only variables of interest
```

There was not much data wrangling needed in order to test my hypothesis. I first renamed X3.points.Field.Goal.Attempted to Threes.Attempted to improve readibility of the code and results. Then, I created a new variable called Twos.Attempted which was the difference between the field goal attempts per game and the three pointer attempts per game. I did this in order to differentiate two pointers from three pointers, since field goal attempts is a combination of the two. Lastly, I selected only the variables of interest in the dataset.

## Conduct

```{r}
model <- lm(Points ~ Minutes.Played + Twos.Attempted + Free.Throws.Attempted + Threes.Attempted + Age, data = df) # Linear regression model with 5 independent variables
```

```{r}
model |>
  modelsummary::modelsummary(stars = TRUE) # Creates a summary table
```


```{r}
coefs <- c("Minutes.Played" = "Minutes Played",
           "Twos.Attepted" = "Two Pointers Attempted",
           "Free.Throws.Attempted" = "Free Throws Attempted",
           "Threes.Attempted" = "Threes Attempted",
           "Age" = "Age") # Labels the coefficients
model |>
  modelsummary::modelplot(coef_map = coefs, coef_omit = 'Intercept') + # Graphs the coefficients of the model
  geom_vline(xintercept = 0, color = 'orange')
```

$$
\hat{y} = -.849791 + .025840\hat{\beta_1} + .985372\hat{\beta_2} + .889580 \hat{\beta_3} + 1.022196(\hat{\beta_4} + .014418\hat{\beta_5} 
$$


$$\beta_1 = Minutes \ Played$$ 
$$\beta_2 = Twos  \ Attempted $$
$$\beta_3 = Free \ Throws \ Attempted $$
$$\beta_4 = Threes\ Attempted $$
$$\beta_5 = Age \ (years) $$
$$

## Interpretations

The R-squared value of .9796 in the model indicates that 97.97% of the sampling variance in Points is explained by the independent variables (Minutes Played, Twos Attempted, Free Throws Attempted, Threes Attempted, and Age). Since the value is close to 1, this model performs very well.

The RSME value of .86 means that the average error performed by the model in predicting the outcome for points per game is .86 points. 

Slope Estimator Coefficients:

Minutes.Played - For every 1 minute increase in minutes played per game, the predicted points per game increases by .02584.

Twos.Attempted - For every 1 attempt increase in two-pointers attempted per game, the predicted points per game increases by .985372.

Free.Throws.Attempted - For every 1 attempt increase in free throws attempted per game, the predicted points per game increases by .889580.

Threes.Attempted - For every 1 attempt increase in three-pointers attempted per game, the predicted points per game increases by 1.022196.

Age - For every 1 year increase in age, the predicted points per game increases by .014418.

Since all of the independent variables in the model have 3 stars next to them in the table, they all have p-values less than .001. This indicates that if there were no relationships between any indiviudal independent variable and points scored per game, there is less than a .1% chance we would observe these patterns. Therefore, we reject the null hypothesis that the coefficients of the independent variables are all equal to 0. There is convincing evidence that all of the explanatory variables have a significant effect in predicting points scored per game. 


## Diagnostics

```{r}
car::vif(model) # Calculates variance inflation factors between independent variables

cor(df) # Prints out correlation between all variables of interest in the dataset

shapiro.test(residuals(model)) # Conducts a shapiro test to see if residuals are normally distributed

plot(model, which = 1) # Creates a residual vs. fitted values scatter plot

sqrt(mean((residuals(model))^2)) # Calculates RMSE
```

While most of the correlation between the independent variables in the model is moderate, there are a couple cases where multicollinearity is an issue. Free Throws Attempted and Twos Attempted have a correlation value of .8474, which indicates that there is multicollinearity between the two variables. Furthermore, Minutes Played and Twos Attempted has a correlation value of .7910, which also indicates multicollinearity between the two. This suggests that the model is mis-specified. Therefore, the Ordinary Least Squares model cannot estimate the marginal effect of Twos Attempted on Points per game while holding Free Throws Attempted and Minutes Played constant because both variables move exactly when Twos Attempted moves.

Since the p-value of the Shapiro-Wilk normality test is very small and significant at the 5% significance level, we reject the null hypothesis that the residuals are normally distributed. This indicates that the residuals of the model are not normally distributed. A variety of issues with the model and/or datset could cause this. There might be missing predictors, variables may need to be transformed, or the wrong test was used.

In the residuals vs. fitted values scatter plot, the variance of the residuals are not constant across the fitted values. This can be seen as the graph is in the shape of a fan. This indicates that the model violates homoskedasticity. Therefore, the OLS estimators are not the best linear unbiased estimators of Points per game. Also, the standard errors of the OLS coefficients are incorrect, which can lead to misleading conclusions in hypothesis testing.

Overall, this model performs well in predicting points scored per game, but may have severe issues because the data did not come from an approximately normal population (violation of homoskedasticity) and there are issues of multicollinearity between the independent variables. Therefore, while the data is appropriate, the model is mis-specified and Ordinary Least Squares is not appropriate. In order to remedy this issue with the model, we could use different independent variables that aren't as correlated in order to predict points scored per game.



